# path: framework/policy_manager.py

import threading
import ray
from typing import Any, Dict, Optional

from compression.controller import CompressionController
from compression.pipeline import CompressionPipeline
from compression.policy import CompressionPolicy
from compression.base import BaseCompressor
from enum import Enum
from ray.rllib.utils.framework import try_import_torch
torch, _ = try_import_torch()


# ============================================================
# Compile Modeï¼ˆä½ çš„åŸå§‹å®šä¹‰ä¿ç•™ï¼‰
# ============================================================
class CompileMode(Enum):
    NONE = "none"
    SYNC = "sync"
    ASYNC = "async"


# ============================================================
# PolicyManager â€”â€” glue RLlib & compression system
# ============================================================
class PolicyManager:
    """
    è´Ÿè´£ï¼š
        - ç®¡ç† pipeline & controllerï¼ˆsync/asyncï¼‰
        - ä» RLlib è®­ç»ƒæ¨¡å‹æŠ½å– backbone
        - æŒ‰ç­–ç•¥è§¦å‘å‹ç¼©
        - å¼‚æ­¥ swap
        - æŠŠ compiled_backbone å¹¿æ’­åˆ°æ‰€æœ‰ rollout workers

    ç”¨æ³•ï¼š
        manager = PolicyManager(algo, compressors, CompileMode.SYNC, trigger_every=5)
        manager.maybe_swap(epoch)
        meta = manager.maybe_trigger(epoch)
    """

    def __init__(self,
                 algo,
                 compressors: [BaseCompressor],
                 mode: CompileMode = CompileMode.NONE,
                 trigger_every: int = 5,
                 enable_diff_check: bool = True,
                 infer_output_index: int = 0,
                 compile_training_backbone: bool = False):

        self.algo = algo
        self.mode = mode

        if not compressors:
            raise ValueError("PolicyManager requires at least one compressor.")
        if infer_output_index < 0 or infer_output_index >= len(compressors):
            raise ValueError("infer_output_index è¶…å‡ºäº† compressors èŒƒå›´ã€‚")
        self._compile_only_mode = (
            len(compressors) == 1
            and compressors[0].__class__.__name__ == "CompileCompressor"
        )
        self._compiled_once = False

        self.compressors = compressors

        # compression policy
        self.policy = CompressionPolicy(
            trigger_every=trigger_every,
            enable_diff_check=enable_diff_check
        )

        # pipeline + controller
        self.pipeline = CompressionPipeline(compressors, self.policy)
        self.model_lock = threading.Lock()
        self.controller = CompressionController(self.pipeline, mode, self.model_lock)

        # RLlib çš„è®­ç»ƒæ¨¡å‹
        self.train_model = self.algo.get_policy().model

        self.infer_output_index = infer_output_index
        self.infer_compressor_name = compressors[infer_output_index].__class__.__name__

        # å½“å‰ sampler æ­£åœ¨ä½¿ç”¨çš„â€œæ¨ç† backboneâ€
        self.current_infer_model: Optional[Any] = None

        # è®°å½•æœ€è¿‘ä¸€æ¬¡å‹ç¼© metadataï¼ˆlatency ç­‰ï¼‰
        self.last_meta: Optional[Dict[str, Any]] = None

        self._compile_training_backbone_flag = compile_training_backbone
        self._training_backbone_compiled = False
        if self._compile_training_backbone_flag and self.mode != CompileMode.NONE:
            self._compile_training_backbone_once()

    # ------------------------------------------------------------------
    # å¹¿æ’­ compiled_backbone åˆ° rollout workers
    # ------------------------------------------------------------------
    def _broadcast_inference_model(self, model):
        """
        å°†ç»™å®š inference backbone è®¾ç½®åˆ°æ‰€æœ‰ rollout worker çš„ policy.model ä¸­ã€‚
        ä½ çš„ CustomPolicyNet éœ€è¦å®ç° set_compiled_backbone()ã€‚
        """
        workers = self.algo.workers.remote_workers()

        def _set(worker):
            def inner(policy, pid):
                if hasattr(policy.model, "set_compiled_backbone"):
                    policy.model.set_compiled_backbone(model)
                return 1
            worker.foreach_policy(inner)
            return 1

        if workers:
            ray.get([w.apply.remote(_set) for w in workers])

        print("[Broadcast] ğŸ“¤ Inference backbone updated on all sampler workers.")

    # ------------------------------------------------------------------
    # å¼‚æ­¥æ¨¡å¼ï¼šåœ¨æ¯ä¸ª epoch å¼€å¤´å°è¯• swapï¼ˆè‹¥å¼‚æ­¥çº¿ç¨‹å·²å®Œæˆï¼‰
    # ------------------------------------------------------------------
    def maybe_swap(self) -> Optional[Dict[str, Any]]:
        if self.mode != CompileMode.ASYNC:
            return None

        outputs, meta = self.controller.try_swap()
        if outputs is None:
            return None

        infer_model = self._select_infer_model(outputs)
        if infer_model is None:
            return None

        self.current_infer_model = infer_model
        self.last_meta = meta

        self._broadcast_inference_model(infer_model)
        if self._compile_only_mode:
            self._compiled_once = True

        print("[AsyncCompile] ğŸ” Swapped inference model.")
        return meta

    # ------------------------------------------------------------------
    # åŒæ­¥/å¼‚æ­¥è§¦å‘å‹ç¼©
    # ------------------------------------------------------------------
    def maybe_trigger(self, epoch: int) -> Optional[Dict[str, Any]]:
        if self.mode == CompileMode.NONE:
            return None
        if self._compile_only_mode and self._compiled_once:
            return None

        # åŒæ­¥æ¨¡å¼ â€”â€” ç«‹å³æ‰§è¡Œ
        if self.mode == CompileMode.SYNC:
            outputs, meta = self.controller.run_sync(self.train_model, epoch)
            if outputs is None:
                return None

            infer_model = self._select_infer_model(outputs)
            if infer_model is None:
                return None

            self.current_infer_model = infer_model
            self.last_meta = meta

            self._broadcast_inference_model(infer_model)
            if self._compile_only_mode:
                self._compiled_once = True

            print("[SyncCompile] âœ… Compiled & swapped immediately.")
            return meta

        # å¼‚æ­¥æ¨¡å¼ â€”â€” è§¦å‘åå°çº¿ç¨‹
        elif self.mode == CompileMode.ASYNC:
            self.controller.trigger_async(self.train_model, epoch)
            return None

        return None

    # ------------------------------------------------------------------
    # è·å–æœ€è¿‘å‹ç¼©ä¿¡æ¯
    # ------------------------------------------------------------------
    def get_last_meta(self):
        return self.last_meta

    # ------------------------------------------------------------------
    # ä¾› Trainer è®¿é—®çš„è¾…åŠ©
    # ------------------------------------------------------------------
    def _select_infer_model(self, outputs):
        if not outputs:
            return None
        if self.infer_output_index >= len(outputs):
            return None
        return outputs[self.infer_output_index]

    def get_infer_compressor_name(self) -> str:
        return self.infer_compressor_name

    # ------------------------------------------------------------------
    # å¯é€‰ï¼šç¼–è¯‘æœ¬åœ°è®­ç»ƒ backbone åŠ é€Ÿå‰å‘
    # ------------------------------------------------------------------
    def _compile_training_backbone_once(self):
        if self._training_backbone_compiled:
            return
        if not hasattr(self.train_model, "backbone"):
            return
        if torch is None:
            return

        backend = "inductor"
        primary = self.compressors[0]
        if hasattr(primary, "backend"):
            backend = getattr(primary, "backend") or backend

        self.train_model.backbone = torch.compile(self.train_model.backbone, backend=backend)
        self._training_backbone_compiled = True
        print(f"[PolicyManager] ğŸ§  Local training backbone compiled via torch.compile backend={backend}.")
