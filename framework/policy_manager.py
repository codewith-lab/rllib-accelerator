# path: framework/policy_manager.py

import threading
import time
import ray
from typing import Any, Dict, Optional

from compression.controller import CompressionController
from compression.pipeline import CompressionPipeline
from compression.policy import CompressionPolicy
from compression.base import BaseCompressor
from enum import Enum
from ray.rllib.utils.framework import try_import_torch
torch, _ = try_import_torch()


# ============================================================
# Compile Modeï¼ˆä½ çš„åŸå§‹å®šä¹‰ä¿ç•™ï¼‰
# ============================================================
class CompileMode(Enum):
    NONE = "none"
    SYNC = "sync"
    ASYNC = "async"


# ============================================================
# PolicyManager â€”â€” glue RLlib & compression system
# ============================================================
class PolicyManager:
    """
    è´Ÿè´£ï¼š
        - ç®¡ç† pipeline & controllerï¼ˆsync/asyncï¼‰
        - ä» RLlib è®­ç»ƒæ¨¡å‹æŠ½å– backbone
        - æŒ‰ç­–ç•¥è§¦å‘å‹ç¼©
        - å¼‚æ­¥ swap
        - æŠŠ compiled_backbone å¹¿æ’­åˆ°æ‰€æœ‰ rollout workers

    ç”¨æ³•ï¼š
        manager = PolicyManager(algo, compressors, CompileMode.SYNC, trigger_every=5)
        manager.maybe_swap(epoch)
        meta = manager.maybe_trigger(epoch)
    """

    def __init__(self,
                 algo,
                 compressors: [BaseCompressor],
                 mode: CompileMode = CompileMode.NONE,
                 trigger_every: int = 5,
                 enable_diff_check: bool = True,
                 infer_output_index: int = 0,
                 compile_training_backbone: bool = False,
                 device: str = "cpu",
                 async_warmup: bool = True):

        self.algo = algo
        self.mode = mode

        if not compressors:
            raise ValueError("PolicyManager requires at least one compressor.")
        if infer_output_index < 0 or infer_output_index >= len(compressors):
            raise ValueError("infer_output_index è¶…å‡ºäº† compressors èŒƒå›´ã€‚")

        self.device = self._resolve_device(device)

        self.compressors = compressors
        self.async_warmup = async_warmup

        # compression policy
        self.policy = CompressionPolicy(
            trigger_every=trigger_every,
            enable_diff_check=enable_diff_check
        )

        # pipeline + controller
        self.pipeline = CompressionPipeline(compressors, self.policy)
        self.model_lock = threading.Lock()
        self.controller = CompressionController(self.pipeline, mode, self.model_lock)

        # RLlib çš„è®­ç»ƒæ¨¡å‹
        self.train_model = self.algo.get_policy().model

        self.infer_output_index = infer_output_index
        self.infer_compressor_name = compressors[infer_output_index].__class__.__name__
        self._supports_weight_sync = bool(
            getattr(compressors[infer_output_index], "supports_weight_sync", False)
        )

        # å½“å‰ sampler æ­£åœ¨ä½¿ç”¨çš„â€œæ¨ç† backboneâ€
        self.current_infer_model: Optional[Any] = None

        # è®°å½•æœ€è¿‘ä¸€æ¬¡å‹ç¼© metadataï¼ˆlatency ç­‰ï¼‰
        self.last_meta: Optional[Dict[str, Any]] = None

        self._compile_training_backbone_flag = compile_training_backbone
        self._training_backbone_compiled = False
        if self._compile_training_backbone_flag:
            self._compile_training_backbone_once()

    # ------------------------------------------------------------------
    # å¹¿æ’­ compiled_backbone åˆ° rollout workers
    # ------------------------------------------------------------------
    def _broadcast_inference_model(self, model, warmup=False, update_only=False):
        """
        å°†ç»™å®š inference backbone è®¾ç½®åˆ°æ‰€æœ‰ rollout worker çš„ policy.model ä¸­ã€‚
        ä½ çš„ CustomPolicyNet éœ€è¦å®ç° set_compiled_backbone()ã€‚
        """
        workers = self.algo.workers.remote_workers()
        state_dict = None
        if update_only and model is not None:
            try:
                state = model.state_dict()
                state_dict = {k: (v.detach().cpu() if torch.is_tensor(v) else v)
                              for k, v in state.items()}
            except Exception as exc:
                print(f"[PolicyManager] âš ï¸ Failed to capture compiled state for update-only swap: {exc}")
                update_only = False
                state_dict = None

        def _set(worker):
            def inner(policy, pid):
                did_update = False
                if update_only and hasattr(policy.model, "update_compiled_backbone_weights") and state_dict is not None:
                    try:
                        policy.model.update_compiled_backbone_weights(state_dict)
                        did_update = True
                    except Exception as exc:
                        print(f"[PolicyManager] âš ï¸ update_only failed on worker, fallback to full swap: {exc}")
                if not did_update and hasattr(policy.model, "set_compiled_backbone"):
                    policy.model.set_compiled_backbone(model)
                    if warmup and hasattr(policy.model, "warmup_compiled_backbone"):
                        policy.model.warmup_compiled_backbone()
                return 1
            worker.foreach_policy(inner)
            return 1

        if workers:
            ray.get([w.apply.remote(_set) for w in workers])

        if not update_only:
            print("[Broadcast] ğŸ“¤ Inference backbone updated on all sampler workers.")

    # ------------------------------------------------------------------
    # å¼‚æ­¥æ¨¡å¼ï¼šåœ¨æ¯ä¸ª epoch å¼€å¤´å°è¯• swapï¼ˆè‹¥å¼‚æ­¥çº¿ç¨‹å·²å®Œæˆï¼‰
    # ------------------------------------------------------------------
    def maybe_swap(self) -> Optional[Dict[str, Any]]:
        if self.mode != CompileMode.ASYNC:
            return None

        outputs, meta = self.controller.try_swap()
        if outputs is None:
            return None

        infer_model = self._select_infer_model(outputs)
        if infer_model is None:
            return None

        self.current_infer_model = infer_model
        self.last_meta = meta

        warmup = (self.mode == CompileMode.ASYNC and self.async_warmup)
        update_only = self._should_update_only(meta)
        t0 = time.time()
        self._broadcast_inference_model(infer_model, warmup=warmup and not update_only, update_only=update_only)
        swap_latency = time.time() - t0
        if meta is None:
            meta = {}
        meta.setdefault("SwapLatency", swap_latency)
        if not update_only:
            print("[AsyncCompile] ğŸ” Swapped inference model.")
        return meta

    def push_weight_update(self):
        """
        å°†è®­ç»ƒæ¨¡å‹æœ€æ–°çš„ backbone æƒé‡åŒæ­¥åˆ°å·²å­˜åœ¨çš„æ¨ç† backboneã€‚
        ä»…å¯¹æ”¯æŒçº¯æƒé‡æ›´æ–°çš„å‹ç¼©å™¨ï¼ˆä¾‹å¦‚ compileï¼‰å¯ç”¨ã€‚
        """
        if not self._supports_weight_sync:
            return
        if self.current_infer_model is None:
            return

        snapshot = self._snapshot_train_backbone()
        if snapshot is None:
            return

        # å…ˆæ›´æ–°æœ¬åœ°æ¨ç†æ¨¡å‹ï¼Œé¿å…ä¸‹ä¸€æ¬¡å¹¿æ’­ä»æ—§æ˜¯æ—§æƒé‡
        self._load_state_into_infer(snapshot)

        workers = self.algo.workers.remote_workers()
        if not workers:
            return

        def _update(worker):
            def inner(policy, pid):
                if hasattr(policy.model, "update_compiled_backbone_weights"):
                    try:
                        policy.model.update_compiled_backbone_weights(snapshot)
                    except Exception as exc:
                        print(f"[PolicyManager] âš ï¸ Weight push failed on worker, skipping: {exc}")
                return 1

            worker.foreach_policy(inner)
            return 1

        ray.get([w.apply.remote(_update) for w in workers])

    # ------------------------------------------------------------------
    # åŒæ­¥/å¼‚æ­¥è§¦å‘å‹ç¼©
    # ------------------------------------------------------------------
    def maybe_trigger(self, epoch: int) -> Optional[Dict[str, Any]]:
        if self.mode == CompileMode.NONE:
            return None
        # åŒæ­¥æ¨¡å¼ â€”â€” ç«‹å³æ‰§è¡Œ
        if self.mode == CompileMode.SYNC:
            outputs, meta = self.controller.run_sync(self.train_model, epoch)
            if outputs is None:
                return None

            infer_model = self._select_infer_model(outputs)
            if infer_model is None:
                return None

            self.current_infer_model = infer_model
            self.last_meta = meta

            update_only = self._should_update_only(meta)
            self._broadcast_inference_model(infer_model, warmup=False, update_only=update_only)
            print("[SyncCompile] âœ… Compiled & swapped immediately.")
            return meta

        # å¼‚æ­¥æ¨¡å¼ â€”â€” è§¦å‘åå°çº¿ç¨‹
        elif self.mode == CompileMode.ASYNC:
            self.controller.trigger_async(self.train_model, epoch)
            return None

        return None

    # ------------------------------------------------------------------
    # è·å–æœ€è¿‘å‹ç¼©ä¿¡æ¯
    # ------------------------------------------------------------------
    def get_last_meta(self):
        return self.last_meta

    # ------------------------------------------------------------------
    # ä¾› Trainer è®¿é—®çš„è¾…åŠ©
    # ------------------------------------------------------------------
    def _select_infer_model(self, outputs):
        if not outputs:
            return None
        if self.infer_output_index >= len(outputs):
            return None
        return outputs[self.infer_output_index]

    def get_infer_compressor_name(self) -> str:
        return self.infer_compressor_name

    def _should_update_only(self, meta: Optional[Dict[str, Any]]) -> bool:
        if not meta:
            return False
        name = self.get_infer_compressor_name()
        info = meta.get(name)
        if not info:
            return False
        return bool(info.get("reused"))

    def _snapshot_train_backbone(self):
        bb = getattr(self.train_model, "backbone", None)
        if bb is None:
            return None
        return {
            k: v.detach().cpu().clone()
            for k, v in bb.state_dict().items()
        }

    def _load_state_into_infer(self, snapshot: Dict[str, Any]):
        if self.current_infer_model is None:
            return
        if torch is None:
            return
        target = getattr(self.current_infer_model, "_orig_mod", self.current_infer_model)
        try:
            device = next(target.parameters()).device
        except StopIteration:
            device = torch.device("cpu")
        converted = {}
        for k, v in snapshot.items():
            if torch.is_tensor(v):
                converted[k] = v.to(device)
            else:
                converted[k] = v
        try:
            target.load_state_dict(converted, strict=False)
        except Exception as exc:
            print(f"[PolicyManager] âš ï¸ Failed to update local inference model: {exc}")

    # ------------------------------------------------------------------
    # å¯é€‰ï¼šç¼–è¯‘æœ¬åœ°è®­ç»ƒ backbone åŠ é€Ÿå‰å‘
    # ------------------------------------------------------------------
    def _compile_training_backbone_once(self):
        if self._training_backbone_compiled:
            return
        if not hasattr(self.train_model, "backbone"):
            return
        if torch is None:
            return

        backend = "inductor"
        primary = self.compressors[0]
        if hasattr(primary, "backend"):
            backend = getattr(primary, "backend") or backend

        if hasattr(self.train_model, "to"):
            self.train_model.to(self.device)

        self.train_model.backbone = torch.compile(self.train_model.backbone, backend=backend)
        self._training_backbone_compiled = True
        print(f"[PolicyManager] ğŸ§  Local training backbone compiled via torch.compile backend={backend}.")

    def _resolve_device(self, device: str):
        if torch is None:
            return "cpu"
        try:
            resolved = torch.device(device)
            if resolved.type.startswith("cuda") and not torch.cuda.is_available():
                print(f"[PolicyManager] âš ï¸ Device {device} unavailable, fallback to CPU.")
                return torch.device("cpu")
            return resolved
        except (RuntimeError, TypeError):
            print(f"[PolicyManager] âš ï¸ Invalid device {device}, fallback to CPU.")
            return torch.device("cpu")
